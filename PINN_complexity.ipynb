{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421a2bf5",
   "metadata": {},
   "source": [
    "## 1 Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af98c59",
   "metadata": {},
   "source": [
    "### 1.1 Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51135b",
   "metadata": {},
   "source": [
    "### 1.2 Connect to machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db739bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eafe09",
   "metadata": {},
   "source": [
    "## 2 Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9414e21",
   "metadata": {},
   "source": [
    "### 2.1 Generate training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(N):\n",
    "    global x\n",
    "    x = torch.zeros([N, N, 2]).to(device)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            x[i][j][0] = i; # x\n",
    "            x[i][j][1] = j; # y\n",
    "    x.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8faa69",
   "metadata": {},
   "source": [
    "### 2.2 Generate benchmark solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FD(N, tol = 1e-6):\n",
    "    global Tn\n",
    "    Tn = np.zeros((N, N))\n",
    "    Tn[:, N - 1] = 1\n",
    "    \n",
    "    h = 1 / (N - 1)\n",
    "    iteration = 0\n",
    "    res = 1\n",
    "    \n",
    "    print(\"tol: \", tol)\n",
    "    \n",
    "    while(res > tol):\n",
    "        iteration = iteration + 1\n",
    "        res = 0\n",
    "        for i in range(1, N - 1):\n",
    "            for j in range(1, N - 1):\n",
    "                prev = Tn[i, j]\n",
    "                Tn[i, j] = Tn[i + 1 , j] + Tn[i - 1, j] + Tn[i, j + 1] + Tn[i, j - 1]\n",
    "                Tn[i, j] = Tn[i, j] * 0.25\n",
    "                res = max(res, np.abs((Tn[i, j] - prev) / (prev + 1e-20))) \n",
    "            \n",
    "        print(\"iteration: \", iteration, \"res: \", res)\n",
    "        \n",
    "    plt.subplots()[1].set_box_aspect(1)\n",
    "    plt.contourf(np.transpose(Tn), vmax = Tn.max(), vmin = Tn.min(), levels = 20)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(17,5))\n",
    "    ax.plot(np.linspace(0, N - 1, N), Tn[:, int((N-1)/2)], '-o')\n",
    "    ax.set(xlim=(0, N))\n",
    "    ax.set_box_aspect(1)\n",
    "    plt.xlabel('j')\n",
    "    plt.ylabel('T')\n",
    "    plt.legend(['FDM', 'PINN'])\n",
    "    plt.xlim((0,N-1))\n",
    "    return Tn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e99e7",
   "metadata": {},
   "source": [
    "### 2.3 PINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401c1f7",
   "metadata": {},
   "source": [
    "##### Variables on the focus:\n",
    "+ pl(neuron \"p\"er \"l\"ayer): 64 by default.\n",
    "+ Depth of NN tunned by commenting out the forward function.\n",
    "+ DAB(\"d\"imension \"a\"nalysis \"b\"ased, i.e weighing scheme):\n",
    "    0. loss weight: same weight | matrix: MSE\n",
    "    1. loss weight: weighted by dimension analyzed result | matrix: MSE\n",
    "    2. loss weight: weighted by root of dimension analyzed result | matrix: MSE\n",
    "    3. loss weight: same weight | matrix: MAE\n",
    "    4. loss weight: weighted by dimension analyzed result | matrix: MAE\n",
    "    5. loss weight: weighted by root of dimension analyzed result | matrix: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_NN(N, w_DE = 1, w_BC = 1, DAB = 0, start_epoch = 0, pl = 64, train_epoch = 50000):\n",
    "    class Net(torch.nn.Module):\n",
    "        def __init__(self, n_feature, n_hidden, n_hidden1, n_hidden2, n_hidden3, n_hidden4, n_hidden5, n_hidden6, n_hidden7, n_hidden8, n_hidden9, n_output):\n",
    "            super(Net, self).__init__()\n",
    "            self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "            self.hidden1 = torch.nn.Linear(n_hidden, n_hidden1)\n",
    "            self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "            self.hidden3 = torch.nn.Linear(n_hidden2, n_hidden3)\n",
    "            self.hidden4 = torch.nn.Linear(n_hidden3, n_hidden4)\n",
    "            self.hidden5 = torch.nn.Linear(n_hidden4, n_hidden5)\n",
    "            self.hidden6 = torch.nn.Linear(n_hidden5, n_hidden6)\n",
    "            self.hidden7 = torch.nn.Linear(n_hidden6, n_hidden7)\n",
    "            self.hidden8 = torch.nn.Linear(n_hidden7, n_hidden8)\n",
    "            self.hidden9 = torch.nn.Linear(n_hidden8, n_hidden9)\n",
    "            self.hidden10 = torch.nn.Linear(n_hidden8, n_hidden9)\n",
    "            self.hidden11 = torch.nn.Linear(n_hidden8, n_hidden9)\n",
    "            self.hidden12 = torch.nn.Linear(n_hidden8, n_hidden9)\n",
    "            self.hidden13 = torch.nn.Linear(n_hidden8, n_hidden9)\n",
    "            self.predict = torch.nn.Linear(n_hidden9, n_output)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.sin(self.hidden(x))\n",
    "            x = torch.sin(self.hidden1(x))\n",
    "            x = torch.sin(self.hidden2(x))\n",
    "            x = torch.sin(self.hidden3(x))\n",
    "            \n",
    "#             x = torch.sin(self.hidden4(x))\n",
    "#             x = torch.sin(self.hidden5(x))\n",
    "#             x = torch.sin(self.hidden6(x))\n",
    "#             x = torch.sin(self.hidden7(x))\n",
    "#             x = torch.sin(self.hidden8(x))\n",
    "\n",
    "#             x = torch.sin(self.hidden9(x))\n",
    "#             x = torch.sin(self.hidden10(x))\n",
    "#             x = torch.sin(self.hidden11(x))\n",
    "#             x = torch.sin(self.hidden12(x))\n",
    "#             x = torch.sin(self.hidden13(x))\n",
    "            \n",
    "            x = self.predict(x)\n",
    "            return x\n",
    "    \n",
    "\n",
    "    net = Net(n_feature=2, n_hidden = pl, n_hidden1 = pl, n_hidden2 = pl, n_hidden3 = pl, n_hidden4 = pl, n_hidden5 = pl, n_hidden6 = pl, n_hidden7 = pl, n_hidden8 = pl, n_hidden9 = pl, n_output=1)     # define the network\n",
    "    net.to(device)\n",
    "    summary(net, (1,1, 2))\n",
    "\n",
    "    ########\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "        \n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    if (DAB == 3)|(DAB == 4)|(DAB == 5):\n",
    "        loss_func = torch.nn.L1Loss()\n",
    "        \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.8)\n",
    "    plt.ion()\n",
    "\n",
    "    ########\n",
    "\n",
    "    if(start_epoch == 0):\n",
    "        loss_epoch=[]\n",
    "        loss_values = []\n",
    "        loss_DE_values = []\n",
    "        loss_DBC_values = []\n",
    "\n",
    "    h = 1 / (N - 1)\n",
    "    loss_op = 'mean'\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # Define loss weight                                          #\n",
    "    ###############################################################\n",
    "\n",
    "    \n",
    "    if DAB == 0: #MSE(1)\n",
    "        b = w_DE\n",
    "        a = w_BC\n",
    "    elif DAB == 1: #MSE(DA)\n",
    "        b = w_DE /((N-1)*(N-1)*(N-1)*(N-1))\n",
    "        a = w_BC \n",
    "    elif DAB == 2: #MSE(Weak)\n",
    "        b = w_DE /((N-1)*(N-1))\n",
    "        a = w_BC \n",
    "    elif DAB == 3: #MAE(1)\n",
    "        b = w_DE\n",
    "        a = w_BC \n",
    "    elif DAB == 4: #MAE(DA)\n",
    "        b = w_DE/((N-1)*(N-1))\n",
    "        a = w_BC \n",
    "    elif DAB == 5: #MAE(Weak)\n",
    "        b = w_DE /(N-1)\n",
    "        a = w_BC \n",
    "    \n",
    "    ###############################################################\n",
    "    # Loss weight normalization                                   #\n",
    "    ###############################################################\n",
    "    \n",
    "    print(b, a)\n",
    "    \n",
    "    lambda_DE = b / (a + b)\n",
    "    lambda_DBC = a / (a + b)\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # Training                                                    #\n",
    "    ###############################################################\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for t in range(start_epoch, train_epoch):\n",
    "    \n",
    "        scheduler.step()\n",
    "        T_out = net(x)\n",
    "        T = T_out[:, :, 0]\n",
    "\n",
    "        lap_T = (-4 * T[1:-1, 1:-1] + T[2:, 1:-1] + T[:-2, 1:-1] + T[1:-1, 2:] + T[1:-1, :-2]) / (h * h)\n",
    "        loss_DE = loss_func(lap_T , torch.zeros((N - 2 , N - 2)).to(device))\n",
    "        loss_DBC = loss_func(T[:, 0], torch.zeros((N , 1)).to(device)) #s\n",
    "        loss_DBC = loss_DBC + loss_func(T[:, -1] - 1, torch.zeros((N , 1)).to(device)) #n\n",
    "        loss_DBC = loss_DBC + loss_func(T[0, :], torch.zeros((1 , N)).to(device)) #w\n",
    "        loss_DBC = loss_DBC + loss_func(T[-1, :], torch.zeros((1 , N)).to(device)) #e\n",
    "\n",
    "        loss =  lambda_DE * loss_DE + lambda_DBC * loss_DBC\n",
    "\n",
    "        loss_epoch.append(t)\n",
    "        loss_DE_values.append(loss_DE.item())\n",
    "        loss_DBC_values.append(loss_DBC.item())\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Epoch: \", t, \" ,loss: \", loss)\n",
    "            print(\"loss_DE: \", loss_DE)\n",
    "            print(\"loss_DBC(*lambda_DBC): \", loss_DBC)\n",
    "            print(\"Timer: {0:.2f} sec\".format(time.perf_counter() - start)) \n",
    "            plt.subplots()[1].set_box_aspect(1)   \n",
    "            plt.cla()\n",
    "            plt.contourf(np.transpose(torch.reshape(T, (N, N)).cpu().detach().numpy()), vmax = T.max(), vmin = T.min())\n",
    "            plt.colorbar()\n",
    "            plt.pause(0.1)     \n",
    "\n",
    "    #####\n",
    "    \n",
    "    print(\"Epoch: \", t, \" ,loss: \", loss)\n",
    "    plt.subplots()[1].set_box_aspect(1)  \n",
    "    plt.cla()\n",
    "    plt.contourf(np.transpose(torch.reshape(T, (N, N)).cpu().detach().numpy()), vmax = Tn.max(), vmin = Tn.min(), levels = 20)\n",
    "    plt.colorbar()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    print(\"Loss curve: \")\n",
    "    plt.plot(loss_epoch, loss_values, loss_epoch, loss_DE_values, loss_epoch, loss_DBC_values)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['$\\mathcal{L}$', '$\\mathcal{L}_{DE}$','$\\mathcal{L}_{DBC}$'])\n",
    "    plt.semilogy()\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    print(\"Error: \")\n",
    "    plt.subplots()[1].set_box_aspect(1)  \n",
    "    plt.cla()\n",
    "    plt.contourf(np.transpose(torch.reshape(T, (N, N)).cpu().detach().numpy() - Tn), vmax = Tn.max(), vmin = Tn.min(), levels = 20)\n",
    "    plt.colorbar()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    print(\"y = 0.5: \")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(17,5))\n",
    "    plot_x = np.linspace(0, N - 1, N)\n",
    "    ax.plot(plot_x, np.transpose(Tn[:, int((N-1)/2)]), plot_x, T[:, int((N-1)/2)].cpu().detach().numpy(), 'r--')\n",
    "    \n",
    "    ax.set(xlim=(0, N - 1))\n",
    "    ax.set_box_aspect(1)\n",
    "    plt.xlabel('j')\n",
    "    plt.ylabel('T')\n",
    "    plt.legend(['Ground truth', 'PINN'])\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    pred = Tn\n",
    "    exact = torch.reshape(T, (N, N)).cpu().detach().numpy()\n",
    "    mse = np.mean(np.power((pred - exact), 2))\n",
    "    \n",
    "    pred_1 = Tn[:, int((N-1)/2)]\n",
    "    exact_1 = torch.reshape(T, (N, N)).cpu().detach().numpy()[:, int((N-1)/2)]\n",
    "    mse_1 = np.mean(np.power((pred_1 - exact_1), 2))\n",
    "\n",
    "    print(f\" ||||| {loss.item() :.4} | {loss_DE.item() :.4} | {loss_DBC.item() :.4} | {time.perf_counter() - start :.2f} sec | {mse :.4} | {mse_1 :.4} |\")\n",
    "    return (torch.reshape(T, (N, N)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66bcef",
   "metadata": {},
   "source": [
    "### 2.4 Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmid(Tn = np.zeros((11, 11)), T1 = np.zeros((11, 11)), T2 = np.zeros((11, 11)), Tx = np.zeros((11, 11)), N = 11):\n",
    "# def plotmid(Tn = np.zeros((11, 11)), T1 = np.zeros((11, 11)), T2 = np.zeros((11, 11)), N = 11):\n",
    "    print(\"x = 0.5: \")\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    plot_x = np.linspace(0, 1, N)\n",
    "    ax.plot(plot_x, (Tn[:, int((N-1)/2)]), '-k', markerfacecolor='none')\n",
    "    ax.plot(plot_x, T1[:, int((N-1)/2)], 'sr', ms = 10, markerfacecolor='none')\n",
    "    ax.plot(plot_x, T2[:, int((N-1)/2)], '^b', ms = 10, markerfacecolor='none')\n",
    "    ax.plot(plot_x, Tx[:, int((N-1)/2)], '+g', ms = 10, markerfacecolor='none')\n",
    "\n",
    "    ax.set(xlim=(0, 1))\n",
    "    ax.set_box_aspect(1)\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('T')\n",
    "    plt.legend(['$T_{FDM}$', '$\\hatT_{1}$', '$\\hatT_{2}$', '$\\hatT_{3}$'], fontsize=\"14\", frameon=False)\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a86bb",
   "metadata": {},
   "source": [
    "## 3. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Sample                                                      #\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154aa30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tn = compute_FD(N, tol= 1e-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04df2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ta = predict_NN(N, pl = 64, DAB = 2, train_epoch = 1000) #101 64*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ff760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tb = predict_NN(N, pl = 128, DAB = 2, train_epoch = 1000) #101 128*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tc = predict_NN(N, pl = 192, DAB = 2, train_epoch = 1000) #101 192*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmid(Tn, Ta, Tb, Tc, N = N) #101 64*5 vs 128*5 vs 192*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945ef70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
